{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN \n",
    "\n",
    "gans are trained to be generative models. They consist of a generator, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Pad\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"Fruitsdata/archive/fruits-360/Training\"\n",
    "\n",
    "\n",
    "#dataset = ImageFolder(data_dir,\n",
    "             \n",
    "             #transform = Compose([ToTensor()]))\n",
    "#data_loader = DataLoader(dataset, batch_size = 1000,shuffle=True)\n",
    "#data = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3081, 0.3638, 0.3936]\n",
      "[0.6813, 0.5756, 0.5041]\n"
     ]
    }
   ],
   "source": [
    "#print(data[0].shape)\n",
    "#mean=torch.mean(data[0],[0,2,3])\n",
    "#std = torch.std(data[0],[0,2,3])\n",
    "std =[0.3081, 0.3638, 0.3936]\n",
    "mean = [0.6813, 0.5756, 0.5041]\n",
    "print(std)\n",
    "print(mean)\n",
    "batch_size =30\n",
    "dataset = ImageFolder(data_dir,\n",
    "             \n",
    "             transform = Compose([ToTensor(),Pad(14,fill=1),Normalize(mean,std)]))\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    if len(x.shape)==3:\n",
    "        for i in range(len(std)):\n",
    "            x[i]*=std[i]\n",
    "            x[i]+= mean[i]\n",
    "    else:\n",
    "        for i in range(len(std)):\n",
    "            for j in range(len(x)):\n",
    "                x[j][i]*=std[i]\n",
    "                x[j][i]+= mean[i]\n",
    "    return x.clamp(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x221e33cb128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "img = dataset[np.random.randint(len(dataset))][0]\n",
    "img_norm = denorm(img)\n",
    "print(img_norm.permute(1,2,0).shape)\n",
    "\n",
    "plt.imshow(img_norm.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It seems we dont need train validation split for this! We use everything for training.\n",
    "\n",
    "Heck we dont even need to have labels! This is unsupervised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in data_loader:\n",
    "    print(xb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator model\n",
    "\n",
    "This takes an input and simply classifies it as real or fake. Use a simple linear fully connected network! \n",
    "The leaky relu allows a small gradieent signal for negative values to be passed through. Makes the gradients from the discriminator flow stronger into the generator. \n",
    "\n",
    "the sigmoid gives a probability - of whether the image was real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): LeakyReLU(negative_slope=0.2)\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (4): LeakyReLU(negative_slope=0.2)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (7): LeakyReLU(negative_slope=0.2)\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (10): LeakyReLU(negative_slope=0.2)\n",
       "  (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  (12): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (13): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(64,64,kernel_size=3,stride=2,padding = 1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.MaxPool2d(2,2),\n",
    "\n",
    "    nn.Conv2d(64,64,kernel_size=3, stride=2, padding = 1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.MaxPool2d(2,2),\n",
    "\n",
    "    nn.Conv2d(64,64, kernel_size=3, stride =2, padding = 1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(256,1),\n",
    "    nn.Sigmoid()   # so that it becomes a probability! \n",
    ")\n",
    "\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
      "         LeakyReLU-5           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          36,928\n",
      "         LeakyReLU-8             [-1, 64, 8, 8]               0\n",
      "         MaxPool2d-9             [-1, 64, 4, 4]               0\n",
      "           Conv2d-10             [-1, 64, 2, 2]          36,928\n",
      "        LeakyReLU-11             [-1, 64, 2, 2]               0\n",
      "          Flatten-12                  [-1, 256]               0\n",
      "           Linear-13                    [-1, 1]             257\n",
      "          Sigmoid-14                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 112,833\n",
      "Trainable params: 112,833\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 19.20\n",
      "Params size (MB): 0.43\n",
      "Estimated Total Size (MB): 19.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(D, input_size = (3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative network  \n",
    "\n",
    "The input to the generator is typically a vector or a matrix, which is used as a seed for generating an image. We will use a linear feedforward neural network with 3 layers. Output will be a vector of 784, which can be transformed into a 28*28 image.\n",
    "\n",
    "Relu has been found to work better for the generative network. \n",
    "\n",
    "Tanh is used because our real images were mapped to the range [-1,1], so we want our generator to generate the same format. Allows the model to learn more quickly and satuates the color space of the training distribution quicker.\n",
    "\n",
    "why `latent_size=64`? This determines how much freedom we have. This for the human face could be that one latent dimension corresponds to the eye color!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorModel(\n",
       "  (linear1): Linear(in_features=64, out_features=4096, bias=True)\n",
       "  (convt1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (convt2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (conv2d): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_size = 64\n",
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(64,256*4*4)\n",
    "        self.convt1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.convt2 = nn.ConvTranspose2d(64,64,kernel_size=4, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2d = nn.Conv2d(64,3, kernel_size=3, padding = 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, xb):\n",
    "        out = self.linear1(xb)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        out = out.reshape(-1,64,8,8)\n",
    "        out = self.convt1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.convt2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.convt2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.convt2(out)\n",
    "        out = self.relu(out)        \n",
    "        \n",
    "        out = self.conv2d(out)\n",
    "        \n",
    "        \n",
    "        out = self.tanh(out)\n",
    "\n",
    "\n",
    "        return out \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "G = GeneratorModel()\n",
    "G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 4096]         266,240\n",
      "              ReLU-2              [-1, 1, 4096]               0\n",
      "   ConvTranspose2d-3           [-1, 64, 16, 16]          65,600\n",
      "              ReLU-4           [-1, 64, 16, 16]               0\n",
      "   ConvTranspose2d-5           [-1, 64, 32, 32]          65,600\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "   ConvTranspose2d-7           [-1, 64, 64, 64]          65,600\n",
      "              ReLU-8           [-1, 64, 64, 64]               0\n",
      "   ConvTranspose2d-9         [-1, 64, 128, 128]          65,600\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "           Conv2d-11          [-1, 3, 128, 128]           1,731\n",
      "             Tanh-12          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 530,371\n",
      "Trainable params: 530,371\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 22.06\n",
      "Params size (MB): 2.02\n",
      "Estimated Total Size (MB): 24.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(G,input_size=(1,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to generate a set of images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = G(torch.randn(1,64).to(device))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22189531e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "gen_imgs = denorm(y.squeeze(0).detach().cpu())\n",
    "plt.imshow(gen_imgs.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatyor Training \n",
    "The discriminator is a binary classification model, we can use the binary crossentropy to use as a loss function. \n",
    "* If the image is real and you predict a high probability then you get a low loss! \n",
    "\n",
    "In this case we are not interested in what the digit contains. We are interested in whether it is real or fake. so labels for real images are all ones, and fake images are all 0s. we do a batch of real then a batch of fake ones.\n",
    "\n",
    "`train_discriminator` does one step of training the discriminator, on a batch of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_opt = torch.optim.Adam(D.parameters(), lr = 0.0002)\n",
    "g_opt = torch.optim.Adam(G.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    d_opt.zero_grad()\n",
    "    g_opt.zero_grad()\n",
    "\n",
    "def train_discriminator(images):\n",
    "    # Create the labels which are later used as input for the BCE loss\n",
    "    \n",
    "    real_labels = torch.ones(images.shape[0], 1).to(device)\n",
    "    fake_labels = torch.zeros(images.shape[0], 1).to(device)\n",
    "    \n",
    "    # loss for real images \n",
    "    outputs = D(images)\n",
    "    d_loss_real = criterion(outputs, real_labels)  # We expect the discriminator to output 1s\n",
    "    real_score = outputs   # These are the probabilities\n",
    "    \n",
    "    # loss for fakeimages \n",
    "    z = torch.randn(batch_size, latent_size).to(device)  # We generate 100  latent vectors and move them to device\n",
    "    fake_images = G(z)     # When we put the latent vectors into the generator, we get 100 fake images\n",
    "    outputs = D(fake_images)   # The discriminator will hopefully think they are 0 \n",
    "    d_loss_fake = criterion(outputs, fake_labels)\n",
    "    fake_score = outputs\n",
    "    # Combine the losses \n",
    "    d_loss = d_loss_real+ d_loss_fake\n",
    "    \n",
    "    reset_grad()\n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    return d_loss, real_score, fake_score\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator training\n",
    "\n",
    "Since the output of the generators are vectors, which can be transformed to images, it is not obvious how we can train the generator. This is where we implement a very elegant **trick**. We do know the output images are generated or fake, we can pass them into the discriminator, and compare the output of the discriminator with the ground truth (i.e. all fake). We can then use this to calculate the loss for the genrator. \n",
    "\n",
    "We use the discriminator as part of the loss function.\n",
    "\n",
    "We wish the discriminator to think all of the generated images are real, so labels of `[1,1,1,1,1...]` is desired. So, the loss function should be the cross entropy between the output and the desired labels, which is a list of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093],\n",
       "        [0.5093]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(batch_size, latent_size).to(device)\n",
    "fake_images = G(z)\n",
    "D(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    z = torch.randn(batch_size, latent_size).to(device)\n",
    "    fake_images = G(z)\n",
    "    labels = torch.ones(batch_size,1).to(device)\n",
    "    g_loss = criterion(D(fake_images),labels) # This is the binary cross entropy. We wish the discriminator to be fooled\n",
    "    \n",
    "    # backprop and optimize\n",
    "    \n",
    "    reset_grad()\n",
    "    g_loss.backward()\n",
    "    g_opt.step()\n",
    "    return g_loss, fake_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "\n",
    "create the directory where we can save intermediate outputs to visually inspect the progress of the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "sample_dir = 'fruitsamples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "for images, _ in data_loader:\n",
    "    images = images.reshape(images.size(0),3,128,128)\n",
    "    save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the generated images before it is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 64\n",
      "Saving fake_images-0002.png\n"
     ]
    }
   ],
   "source": [
    "G.load_state_dict(torch.load(\"gan_cnn1G.pth\"))\n",
    "D.load_state_dict(torch.load(\"gan_cnn1D.pth\"))\n",
    "\n",
    "sample_vectors = torch.randn(batch_size, latent_size).to(device)\n",
    "print(batch_size, latent_size)\n",
    "torch.cuda.empty_cache()\n",
    "def save_fake_images(index):\n",
    "    fake_images = G(sample_vectors)\n",
    "    fake_images = fake_images.reshape(fake_images.size(0),3,128,128)\n",
    "    fake_fname= 'fake_images-{0:0=4d}.png'.format(index)\n",
    "    print('Saving', fake_fname)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=5)\n",
    "save_fake_images(2)\n",
    "Image(os.path.join(sample_dir,'fake_images-0000.png'))\n",
    "del fake_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_current_ite():\n",
    "    current_ite = os.listdir(sample_dir)\n",
    "    current_ite = [int(x[-8:-4]) for x in current_ite if \"fake_images\" in x]\n",
    "    current_ite = max(current_ite)\n",
    "    return current_ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Epoch 1 / 3, step 200/2256, d_loss: 0.0000, g_loss: 13.2829, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0036.png\n",
      "Epoch 1 / 3, step 400/2256, d_loss: 0.0000, g_loss: 13.3552, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0037.png\n",
      "Epoch 1 / 3, step 600/2256, d_loss: 0.0000, g_loss: 12.8770, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0038.png\n",
      "Epoch 1 / 3, step 800/2256, d_loss: 0.0000, g_loss: 13.1386, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0039.png\n",
      "Epoch 1 / 3, step 1000/2256, d_loss: 0.0000, g_loss: 13.0953, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0040.png\n",
      "Epoch 1 / 3, step 1200/2256, d_loss: 0.0000, g_loss: 13.4051, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0041.png\n",
      "Epoch 1 / 3, step 1400/2256, d_loss: 0.0000, g_loss: 13.8196, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0042.png\n",
      "Epoch 1 / 3, step 1600/2256, d_loss: 0.0000, g_loss: 14.0797, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0043.png\n",
      "Epoch 1 / 3, step 1800/2256, d_loss: 0.0000, g_loss: 13.6104, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0044.png\n",
      "Epoch 1 / 3, step 2000/2256, d_loss: 0.0000, g_loss: 13.8393, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0045.png\n",
      "Epoch 1 / 3, step 2200/2256, d_loss: 0.0000, g_loss: 14.1247, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0046.png\n",
      "Epoch 2 / 3, step 200/2256, d_loss: 0.0000, g_loss: 13.7597, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0047.png\n",
      "Epoch 2 / 3, step 400/2256, d_loss: 0.0000, g_loss: 14.2103, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0048.png\n",
      "Epoch 2 / 3, step 600/2256, d_loss: 0.0000, g_loss: 14.6089, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0049.png\n",
      "Epoch 2 / 3, step 800/2256, d_loss: 0.0000, g_loss: 14.6243, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0050.png\n",
      "Epoch 2 / 3, step 1000/2256, d_loss: 0.0000, g_loss: 15.0893, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0051.png\n",
      "Epoch 2 / 3, step 1200/2256, d_loss: 0.0000, g_loss: 15.1006, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0052.png\n",
      "Epoch 2 / 3, step 1400/2256, d_loss: 0.0000, g_loss: 15.4183, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0053.png\n",
      "Epoch 2 / 3, step 1600/2256, d_loss: 0.0000, g_loss: 14.8882, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0054.png\n",
      "Epoch 2 / 3, step 1800/2256, d_loss: 0.0000, g_loss: 15.6800, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0055.png\n",
      "Epoch 2 / 3, step 2000/2256, d_loss: 0.0000, g_loss: 15.5933, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0056.png\n",
      "Epoch 2 / 3, step 2200/2256, d_loss: 0.0000, g_loss: 15.9298, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0057.png\n",
      "Epoch 3 / 3, step 200/2256, d_loss: 0.0000, g_loss: 15.5260, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0058.png\n",
      "Epoch 3 / 3, step 400/2256, d_loss: 0.0000, g_loss: 16.1973, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0059.png\n",
      "Epoch 3 / 3, step 600/2256, d_loss: 0.0000, g_loss: 16.2729, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0060.png\n",
      "Epoch 3 / 3, step 800/2256, d_loss: 0.0000, g_loss: 16.4696, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0061.png\n",
      "Epoch 3 / 3, step 1000/2256, d_loss: 0.0000, g_loss: 16.7486, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0062.png\n",
      "Epoch 3 / 3, step 1200/2256, d_loss: 0.0000, g_loss: 16.8338, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0063.png\n",
      "Epoch 3 / 3, step 1400/2256, d_loss: 0.0000, g_loss: 16.9572, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0064.png\n",
      "Epoch 3 / 3, step 1600/2256, d_loss: 0.0000, g_loss: 17.1109, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0065.png\n",
      "Epoch 3 / 3, step 1800/2256, d_loss: 0.0000, g_loss: 17.3398, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0066.png\n",
      "Epoch 3 / 3, step 2000/2256, d_loss: 0.0000, g_loss: 17.3571, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0067.png\n",
      "Epoch 3 / 3, step 2200/2256, d_loss: 0.0000, g_loss: 17.7960, D(x): 1.00, D(g(z)): 0.00\n",
      "Saving fake_images-0068.png\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 3\n",
    "total_step = len(data_loader)\n",
    "current = find_current_ite()\n",
    "print(batch_size)\n",
    "from tqdm import tqdm \n",
    "torch.cuda.empty_cache()\n",
    "d_losses, g_losses, real_scores, fake_scores = [],[],[],[]\n",
    "counter = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # load a batch and transform to vectors \n",
    "        images = images.to(device)\n",
    "        d_loss, real_score, fake_score = train_discriminator(images)\n",
    "        g_loss, fake_images = train_generator()\n",
    "        \n",
    "        # inspect the losses \n",
    "        if (i+1) %200 ==0:\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            real_scores.append(real_score.mean().item())\n",
    "            fake_scores.append(fake_score.mean().item())\n",
    "            print(\"Epoch %d / %d, step %d/%d, d_loss: %.4f, g_loss: %.4f, D(x): %.2f, D(g(z)): %.2f\" %(epoch+1, num_epochs, i+1, total_step, \n",
    "                                                                                     d_loss, g_loss, real_score.mean(),\n",
    "                                                                                                      fake_score.mean()))\n",
    "            save_fake_images(counter+current)\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(D.state_dict(), \"gan_cnn1D.pth\")\n",
    "torch.save(G.state_dict(), 'gan_cnn1G.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results \n",
    "\n",
    "It will be nice to actually visualise the progress using a video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os \n",
    "from IPython.display.display import FileLink\n",
    "vid_fname = 'gans_training.avi'\n",
    "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'fake_images' in f]\n",
    "files.sort()\n",
    "out = cv2.VideoWriter(vid_fname, cv2.VideoWriter_fourcc(*'MP4V'), 8, (782,652))  # 8 images per second. (302,302) is the combined size of the images\n",
    "[out.write(cv2.imread(fname)) for fname in files]\n",
    "out.release()\n",
    "FileLink('gans_training.avi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/133 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ea9ba0680ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mall_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/all\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mfruitdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#print(os.listdir(data_dir))\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "\n",
    "#os.mkdir(data_dir+\"/all\")\n",
    "all_dir = data_dir+\"/all\"\n",
    "for d in tqdm(os.listdir(data_dir)):\n",
    "    if d != \"all\":\n",
    "        fruitdir = data_dir+\"/\"+d\n",
    "        imgs = os.listdir(data_dir+\"/\"+d)\n",
    "        for img in imgs:\n",
    "            imgdir = fruitdir+\"/\"+img\n",
    "            now = str(datetime.datetime.now())[:19]\n",
    "            now = now.replace(\":\",\"_\")\n",
    "            newdir = all_dir +\"/\"+ str(np.random.randint(10000000)) +\".jpg\"\n",
    "            shutil.copy(imgdir,newdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
